<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
    <title>Clever Algorithms</title>
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    
<a id='immune_network_algorithm'><h1>Immune Network Algorithm</h1></a>
<p>
<em>Artificial Immune Network, aiNet, Optimization Artificial Immune Network, opt-aiNet.</em>
</p>

<a id='taxonomy'><h2>Taxonomy</h2></a>
<p>
The Artificial Immune Network algorithm (aiNet) is a Immune Network Algorithm from the field of Artificial Immune Systems.
It is related to other Artificial Immune System algorithms such as the Clonal Selection Algorithm, the Negative Selection Algorithm, and the Dendritic Cell Algorithm.
The Artificial Immune Network algorithm includes the base version and the extension for optimization problems called the Optimization Artificial Immune Network algorithm (opt-aiNet).
</p>


<a id='inspiration'><h2>Inspiration</h2></a>
<p>
The Artificial Immune Network algorithm is inspired by the Immune Network theory of the acquired immune system.
The clonal selection theory of acquired immunity accounts for the adaptive behavior of the immune system including the ongoing selection and proliferation of cells that select-for potentially harmful (and typically foreign) material in the body.
A concern of the clonal selection theory is that it presumes that the repertoire of reactive cells remains idle when there are no pathogen to which to respond. Jerne proposed an Immune Network Theory (Idiotypic Networks) where immune cells are not at rest in the absence of pathogen, instead antibody and immune cells recognize and respond to each other  [<a href='#Jerne1974'>Jerne1974</a>] [<a href='#Jerne1974a'>Jerne1974a</a>] [<a href='#Jerne1984'>Jerne1984</a>].
</p>
<p>
The Immune Network theory proposes that antibody (both free floating and surface bound) possess idiotopes (surface features) to which the receptors of other antibody can bind. As a result of receptor interactions, the repertoire becomes dynamic, where receptors continually both inhibit and excite each other in complex regulatory networks (chains of receptors). The theory suggests that the clonal selection process may be triggered by the idiotopes of other immune cells and molecules in addition to the surface characteristics of pathogen, and that the maturation process applies both to the receptors themselves and the idiotopes which they expose.
</p>


<a id='metaphor'><h2>Metaphor</h2></a>
<p>
The immune network theory has interesting resource maintenance and signaling information processing properties.
The classical clonal selection and negative selection paradigms integrate the accumulative and filtered learning of the acquired immune system, whereas the immune network theory proposes an additional order of complexity between the cells and molecules under selection. In addition to cells that interact directly with pathogen, there are cells that interact with those reactive cells and with pathogen indirectly, in successive layers such that networks of activity for higher-order structures such as internal images of pathogen (promotion), and regulatory networks (so-called anti-idiotopes and anti-anti-idiotopes).
</p>


<a id='strategy'><h2>Strategy</h2></a>
<p>
The objective of the immune network process is to prepare a repertoire of discrete pattern detectors for a given problem domain, where better performing cells suppress low-affinity (similar) cells in the network.
This principle is achieved through an interactive process of exposing the population to external information to which it responds with both a clonal selection response and internal meta-dynamics of intra-population responses that stabilizes the responses of the population to the external stimuli.
</p>


<a id='procedure'><h2>Procedure</h2></a>
<p>
Algorithm (below) provides a pseudocode listing of the Optimization Artificial Immune Network algorithm (opt-aiNet) for minimizing a cost function.
</p>
<div class='pseudocode'>
<strong><strong><code>Input</code></strong></strong>: 
$Population_{size}$, <code>ProblemSize</code>, $N_{clones}$, $N_{random}$, <code>AffinityThreshold</code>
<br />
<strong><strong><code>Output</code></strong></strong>: 
$S_{best}$
<br />
<code>Population</code> $\leftarrow$ <code>InitializePopulation</code>($Population_{size}$, <code>ProblemSize</code>)<br />
<strong><code>While</code></strong> ($\neg$<code>StopCondition</code>())<br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>EvaluatePopulation</code>(<code>Population</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$S_{best}$ $\leftarrow$ <code>GetBestSolution</code>(<code>Population</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>Progeny</code> $\leftarrow \emptyset$<br />
&nbsp;&nbsp;&nbsp;&nbsp;$Cost_{avg}$ $\leftarrow$ <code>CalculateAveragePopulationCost</code>(<code>Population</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>While</code></strong> (<code>CalculateAveragePopulationCost</code>(<code>Population</code>) &gt; $Cost_{avg}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>For</code></strong> ($Cell_{i}$ $\in$ <code>Population</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Clones</code> $\leftarrow$ <code>CreateClones</code>($Cell_{i}$, $N_{clones}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>For</code></strong> ($Clone_{i}$ $\in$ <code>Clones</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Clone_{i}$ $\leftarrow$ <code>MutateRelativeToFitnessOfParent</code>($Clone_{i}$, $Cell_{i}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>EvaluatePopulation</code>(<code>Clones</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<code>Progeny</code> $\leftarrow$ <code>GetBestSolution</code>(<code>Clones</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>SupressLowAffinityCells</code>(<code>Progeny</code>, <code>AffinityThreshold</code>)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>Progeny</code> $\leftarrow$ <code>CreateRandomCells</code>($N_{random}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<code>Population</code> $\leftarrow$ <code>Progeny</code><br />
<strong><code>End</code></strong><br />
<strong><code>Return</code></strong> ($S_{best}$)<br />
</div>
<div class='caption'>Pseudocode for opt-aiNet.</div>



<a id='heuristics'><h2>Heuristics</h2></a>
<ul>
<li> aiNet is designed for unsupervised clustering, where as the opt-aiNet extension was designed for pattern recognition and optimization, specifically multi-modal function optimization.</li>
<li> The amount of mutation of clones is proportionate to the affinity of the parent cell with the cost function (better fitness, lower mutation).</li>
<li> The addition of random cells each iteration adds a random-restart like capability to the algorithms.</li>
<li> Suppression based on cell similarity provides a mechanism for reducing redundancy.</li>
<li> The population size is dynamic, and if it continues to grow it may be an indication of a problem with many local optima or that the affinity threshold may needs to be increased.</li>
<li> Affinity proportionate mutation is performed using $c' = c + \alpha \times N(1,0)$ where $\alpha = \frac{1}{\beta} \times exp(-f)$, $N$ is a Guassian random number, and $f$ is the fitness of the parent cell, $\beta$ controls the decay of the function and can be set to 100.</li>
<li> The affinity threshold is problem and representation specific, for example a $AffinityThreshold$ may be set to an arbitrary value such as 0.1 on a continuous function domain, or calculated as a percentage of the size of the problem space.</li>
<li> The number of random cells inserted may be 40% of the population size.</li>
<li> The number of clones created for a cell may be small, such as 10.</li>
</ul>


<a id='code_listing'><h2>Code Listing</h2></a>
<p>
Listing (below) provides an example of the Optimization Artificial Immune Network (opt-aiNet) implemented in the Ruby Programming Language.
The demonstration problem is an instance of a continuous function optimization that seeks $\min f(x)$ where $f=\sum_{i=1}^n x_{i}^2$, $-5.0\leq x_i \leq 5.0$ and $n=2$. The optimal solution for this basin function is $(v_0,\ldots,v_{n-1})=0.0$.
The algorithm is an implementation based on the specification by de Castro and Von Zuben  [<a href='#Castro2002c'>Castro2002c</a>].
</p>
<pre class='prettyprint lang-rb'>
def objective_function(vector)
  return vector.inject(0.0) {|sum, x| sum + (x**2.0)}
end

def random_vector(minmax)
  return Array.new(minmax.size) do |i|
    minmax[i][0] + ((minmax[i][1] - minmax[i][0]) * rand())
  end
end

def random_gaussian(mean=0.0, stdev=1.0)
  u1 = u2 = w = 0
  begin
    u1 = 2 * rand() - 1
    u2 = 2 * rand() - 1
    w = u1 * u1 + u2 * u2
  end while w &gt;= 1
  w = Math.sqrt((-2.0 * Math.log(w)) / w)
  return mean + (u2 * w) * stdev
end

def clone(parent)
  v = Array.new(parent[:vector].size) {|i| parent[:vector][i]}
  return {:vector=&gt;v}
end

def mutation_rate(beta, normalized_cost)
  return (1.0/beta) * Math.exp(-normalized_cost)
end

def mutate(beta, child, normalized_cost)
  child[:vector].each_with_index do |v, i|
    alpha = mutation_rate(beta, normalized_cost)
    child[:vector][i] = v + alpha * random_gaussian()
  end
end

def clone_cell(beta, num_clones, parent)
  clones = Array.new(num_clones) {clone(parent)}
  clones.each {|clone| mutate(beta, clone, parent[:norm_cost])}
  clones.each{|c| c[:cost] = objective_function(c[:vector])}
  clones.sort!{|x,y| x[:cost] &lt;=&gt; y[:cost]}
  return clones.first
end

def calculate_normalized_cost(pop)
  pop.sort!{|x,y| x[:cost]&lt;=&gt;y[:cost]}
  range = pop.last[:cost] - pop.first[:cost]
  if range == 0.0
    pop.each {|p| p[:norm_cost] = 1.0}
  else
    pop.each {|p| p[:norm_cost] = 1.0-(p[:cost]/range)}
  end
end

def average_cost(pop)
  sum = pop.inject(0.0){|sum,x| sum + x[:cost]}
  return sum / pop.size.to_f
end

def distance(c1, c2)
  sum = 0.0
  c1.each_index {|i| sum += (c1[i]-c2[i])**2.0}
  return Math.sqrt(sum)
end

def get_neighborhood(cell, pop, aff_thresh)
  neighbors = []
  pop.each do |p|
    neighbors &lt;&lt; p if distance(p[:vector], cell[:vector]) &lt; aff_thresh
  end
  return neighbors
end

def affinity_supress(population, aff_thresh)
  pop = []
  population.each do |cell|
    neighbors = get_neighborhood(cell, population, aff_thresh)
    neighbors.sort!{|x,y| x[:cost] &lt;=&gt; y[:cost]}
    pop &lt;&lt; cell if neighbors.empty? or cell.equal?(neighbors.first)
  end
  return pop
end

def search(search_space, max_gens, pop_size, num_clones, beta, num_rand, aff_thresh)
  pop = Array.new(pop_size) {|i| {:vector=&gt;random_vector(search_space)} }
  pop.each{|c| c[:cost] = objective_function(c[:vector])}
  best = nil
  max_gens.times do |gen|
    pop.each{|c| c[:cost] = objective_function(c[:vector])}
    calculate_normalized_cost(pop)
    pop.sort!{|x,y| x[:cost] &lt;=&gt; y[:cost]}
    best = pop.first if best.nil? or pop.first[:cost] &lt; best[:cost]
    avgCost, progeny = average_cost(pop), nil
    begin
      progeny=Array.new(pop.size){|i| clone_cell(beta, num_clones, pop[i])}
    end until average_cost(progeny) &lt; avgCost
    pop = affinity_supress(progeny, aff_thresh)
    num_rand.times {pop &lt;&lt; {:vector=&gt;random_vector(search_space)}}
    puts " &gt; gen #{gen+1}, popSize=#{pop.size}, fitness=#{best[:cost]}"
  end
  return best
end

if __FILE__ == $0
  # problem configuration
  problem_size = 2
  search_space = Array.new(problem_size) {|i| [-5, +5]}
  # algorithm configuration
  max_gens = 150
  pop_size = 20
  num_clones = 10
  beta = 100
  num_rand = 2
  aff_thresh = (search_space[0][1]-search_space[0][0])*0.05
  # execute the algorithm
  best = search(search_space, max_gens, pop_size, num_clones, beta, num_rand, aff_thresh)
  puts "done! Solution: f=#{best[:cost]}, s=#{best[:vector].inspect}"
end
</pre>
<div class='download_src'><a href='optainet.rb'>Download Source</a></div>
<div class='caption'>Optimization Artificial Immune Network in Ruby</div>


<a id='references'><h2>References</h2></a>

<a id='primary_sources'><h3>Primary Sources</h3></a>
<p>
Early works, such as Farmer et al.  [<a href='#Farmer1986'>Farmer1986</a>] suggested at the exploitation of the information processing properties of network theory for machine learning.
A seminal network theory based algorithm was proposed by Timmis et al. for clustering problems called the Artificial Immune Network (AIN)  [<a href='#Timmis2000'>Timmis2000</a>] that was later extended and renamed the Resource Limited Artificial Immune System  [<a href='#Timmis2001'>Timmis2001</a>] and Artificial Immune Network (AINE)  [<a href='#Knight2001'>Knight2001</a>].
The Artificial Immune Network (aiNet) algorithm was proposed by de Castro and Von Zuben that extended the principles of the Artificial Immune Network (AIN) and the Clonal Selection Algorithm (CLONALG) and was applied to clustering  [<a href='#Castro2000a'>Castro2000a</a>]. The aiNet algorithm was further extended to optimization domains and renamed opt-aiNet  [<a href='#Castro2002c'>Castro2002c</a>].
</p>


<a id='learn_more'><h3>Learn More</h3></a>
<p>
The authors de Castro and Von Zuben provide a detailed presentation of the aiNet algorithm as a book chapter that includes immunological theory, a description of the algorithm, and demonstration application to clustering problem instances  [<a href='#Castro2001'>Castro2001</a>].
Timmis and Edmonds provide a careful examination of the opt-aiNet algorithm and propose some modifications and augmentations to improve its applicability and performance for multimodal function optimization problem domains  [<a href='#Timmis2004'>Timmis2004</a>].
The authors de Franca, Von Zuben, and de Castro proposed an extension to opt-aiNet that provided a number of enhancements and adapted its capability for for dynamic function optimization problems called dopt-aiNet  [<a href='#Franca2005'>Franca2005</a>].
</p>




<h2>Bibliography</h2>
<table>
 <tr valign="top">
 <td><a id='Castro2000a'>[Castro2000a]</a></td>
 <td>L. N. de Castro and F. J. Von Zuben, "<a href="http://scholar.google.com.au/scholar?q=An+evolutionary+immune+network+for+data+clustering">An evolutionary immune network for data clustering</a>", in Proceedings Sixth Brazilian Symposium on Neural Networks, 2000.</td>
 </tr>
 <tr valign="top">
 <td><a id='Castro2001'>[Castro2001]</a></td>
 <td>L. N. de Castro and F. J. Von Zuben, "<a href="http://scholar.google.com.au/scholar?q=Chapter+XII:+aiNet:+An+Artificial+Immune+Network+for+Data+Analysis">Chapter XII: aiNet: An Artificial Immune Network for Data Analysis</a>", in Data Mining: A Heuristic Approach, pages 231&ndash;259, Idea Group Publishing, 2001.</td>
 </tr>
 <tr valign="top">
 <td><a id='Castro2002c'>[Castro2002c]</a></td>
 <td>L. N. de Castro and J. Timmis, "<a href="http://scholar.google.com.au/scholar?q=An+artificial+immune+network+for+multimodal+function+optimization">An artificial immune network for multimodal function optimization</a>", in Proceedings of the 2002 Congress on Evolutionary Computation (CEC'02), 2002.</td>
 </tr>
 <tr valign="top">
 <td><a id='Farmer1986'>[Farmer1986]</a></td>
 <td>J. D. Farmer and N. H. Packard and Alan S. Perelson, "<a href="http://scholar.google.com.au/scholar?q=The+immune+system,+adaptation,+and+machine+learning">The immune system, adaptation, and machine learning</a>", Physica D, 1986.</td>
 </tr>
 <tr valign="top">
 <td><a id='Franca2005'>[Franca2005]</a></td>
 <td>F. O. de Fran&ccedil;a and L. N. de Castro and F. J. Von Zuben, "<a href="http://scholar.google.com.au/scholar?q=An+artificial+immune+network+for+multimodal+function+optimization++on+dynamic+environments">An artificial immune network for multimodal function optimization 	on dynamic environments</a>", in Genetic And Evolutionary Computation Conference, 2005.</td>
 </tr>
 <tr valign="top">
 <td><a id='Jerne1974'>[Jerne1974]</a></td>
 <td>N. K. Jerne, "<a href="http://scholar.google.com.au/scholar?q=Clonal+selection+in+a+lymphocyte+network">Clonal selection in a lymphocyte network</a>", in Cellular Selection and Regulation in the Immune Response, 1974.</td>
 </tr>
 <tr valign="top">
 <td><a id='Jerne1974a'>[Jerne1974a]</a></td>
 <td>N. K. Jerne, "<a href="http://scholar.google.com.au/scholar?q=Towards+a+network+theory+of+the+immune+system">Towards a network theory of the immune system</a>", Annales d'immunologie (Annals of Immunology), Institut Pasteur (Paris, 	France), Societe Francaise d'Immunologie, 1974.</td>
 </tr>
 <tr valign="top">
 <td><a id='Jerne1984'>[Jerne1984]</a></td>
 <td>N. K. Jerne, "<a href="http://scholar.google.com.au/scholar?q=Idiotypic+networks+and+other+preconceived+ideas">Idiotypic networks and other preconceived ideas</a>", Immunological Reviews, 1984.</td>
 </tr>
 <tr valign="top">
 <td><a id='Knight2001'>[Knight2001]</a></td>
 <td>T. Knight and J. Timmis, "<a href="http://scholar.google.com.au/scholar?q=AINE:+An+Immunological+Approach+to+Data+Mining">AINE: An Immunological Approach to Data Mining</a>", in First IEEE International Conference on Data Mining (ICDM'01), 2001.</td>
 </tr>
 <tr valign="top">
 <td><a id='Timmis2000'>[Timmis2000]</a></td>
 <td>J. Timmis and M. Neal and J. Hunt, "<a href="http://scholar.google.com.au/scholar?q=An+Artificial+Immune+System+for+Data+Analysis">An Artificial Immune System for Data Analysis</a>", Biosystems, 2000.</td>
 </tr>
 <tr valign="top">
 <td><a id='Timmis2001'>[Timmis2001]</a></td>
 <td>J. Timmis and M. J. Neal, "<a href="http://scholar.google.com.au/scholar?q=A+resource+limited+artificial+immune+system+for+data+analysis">A resource limited artificial immune system for data analysis</a>", Knowledge Based Systems Journal: Special Issue, 2001.</td>
 </tr>
 <tr valign="top">
 <td><a id='Timmis2004'>[Timmis2004]</a></td>
 <td>J. Timmis and C. Edmonds, "<a href="http://scholar.google.com.au/scholar?q=A+Comment+on+opt&ndash;AINet:+An+Immune+Network+Algorithm+for+Optimisation">A Comment on opt&ndash;AINet: An Immune Network Algorithm for Optimisation</a>", in Lecture Notes in Computer Science, 2004.</td>
 </tr>
</table>


  </body>
</html>  
