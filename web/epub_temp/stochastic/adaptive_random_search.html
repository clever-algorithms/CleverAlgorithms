<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
    <title>Clever Algorithms</title>
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    
<a id='adaptive_random_search'><h1>Adaptive Random Search</h1></a>
<p>
<em>Adaptive Random Search, ARS, Adaptive Step Size Random Search, ASSRS, Variable Step-Size Random Search.</em>
</p>

<a id='taxonomy'><h2>Taxonomy</h2></a>
<p>
The Adaptive Random Search algorithm belongs to the general set of approaches known as Stochastic Optimization and Global Optimization. It is a direct search method in that it does not require derivatives to navigate the search space.
Adaptive Random Search is an extension of the Random Search and Localized Random Search algorithms.
</p>


<a id='strategy'><h2>Strategy</h2></a>
<p>
The Adaptive Random Search algorithm was designed to address the limitations of the fixed step size in the Localized Random Search algorithm. The strategy for Adaptive Random Search is to continually approximate the optimal step size required to reach the global optimum in the search space. This is achieved by trialling and adopting smaller or larger step sizes only if they result in an improvement in the search performance.
</p>
<p>
The Strategy of the Adaptive Step Size Random Search algorithm (the specific technique reviewed) is to trial a larger step in each iteration and adopt the larger step if it results in an improved result. Very large step sizes are trialled in the same manner although with a much lower frequency. This strategy of preferring large moves is intended to allow the technique to escape local optima. Smaller step sizes are adopted if no improvement is made for an extended period.
</p>


<a id='procedure'><h2>Procedure</h2></a>
<p>
Algorithm (below) provides a pseudocode listing of the Adaptive Random Search Algorithm for minimizing a cost function based on the specification for 'Adaptive Step-Size Random Search' by Schummer and Steiglitz  [<a href='#Schumer1968'>Schumer1968</a>].
</p>
<div class='pseudocode'>
<strong><strong><code>Input</code></strong></strong>: 
$Iter_{max}$, $Problem_{size}$, <code>SearchSpace</code>, $StepSize_{factor}^{init}$, $StepSize_{factor}^{small}$, $StepSize_{factor}^{large}$, $StepSize_{factor}^{iter}$, $NoChange_{max}$
<br />
<strong><strong><code>Output</code></strong></strong>: 
$S$
<br />
$NoChange_{count}$ $\leftarrow$ 0<br />
$StepSize_{i}$ $\leftarrow$ <code>InitializeStepSize</code>(<code>SearchSpace</code>, $StepSize_{factor}^{init}$)<br />
$S$ $\leftarrow$ <code>RandomSolution</code>($Problem_{size}$, <code>SearchSpace</code>)<br />
<strong><code>For</code></strong> ($i=0$ <strong><code>To</code></strong> $Iter_{max}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$S_1$ $\leftarrow$ <code>TakeStep</code>(<code>SearchSpace</code>, $S$, $StepSize_{i}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$StepSize_{i}^{large}$ $\leftarrow$ 0<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> ($i$ $\bmod{StepSize_{factor}^{iter}}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$StepSize_{i}^{large}$ $\leftarrow$ $StepSize_{i}$ $\times$ $StepSize_{factor}^{large}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>Else</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$StepSize_{i}^{large}$ $\leftarrow$ $StepSize_{i}$ $\times$ $StepSize_{factor}^{small}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;$S_2$ $\leftarrow$ <code>TakeStep</code>(<code>SearchSpace</code>, $S$, $StepSize_{i}^{large}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> (<code>Cost</code>($S_1$)$\leq$<code>Cost</code>($S$) || <code>Cost</code>($S_2$)$\leq$<code>Cost</code>($S$))<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> (<code>Cost</code>($S_2$)&lt;<code>Cost</code>($S_1$))<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S$ $\leftarrow$ $S_2$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$StepSize_{i}$ $\leftarrow$ $StepSize_{i}^{large}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>Else</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S$ $\leftarrow$ $S_1$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$NoChange_{count}$ $\leftarrow$ 0<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>Else</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$NoChange_{count}$ $\leftarrow$ $NoChange_{count}$ + 1<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> ($NoChange_{count}$ &gt; $NoChange_{max}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$NoChange_{count}$ $\leftarrow$ 0<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$StepSize_{i}$ $\leftarrow$ $\frac{StepSize_{i}}{StepSize_{factor}^{small}}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
<strong><code>End</code></strong><br />
<strong><code>Return</code></strong> ($S$)<br />
</div>
<div class='caption'>Pseudocode for Adaptive Random Search.</div>



<a id='heuristics'><h2>Heuristics</h2></a>
<ul>
<li> Adaptive Random Search was designed for continuous function optimization problem domains.</li>
<li> Candidates with equal cost should be considered improvements to allow the algorithm to make progress across plateaus in the response surface.</li>
<li> Adaptive Random Search may adapt the search direction in addition to the step size.</li>
<li> The step size may be adapted for all parameters, or for each parameter individually.</li>
</ul>


<a id='code_listing'><h2>Code Listing</h2></a>
<p>
Listing (below) provides an example of the Adaptive Random Search Algorithm implemented in the Ruby Programming Language, based on the specification for 'Adaptive Step-Size Random Search' by Schummer and Steiglitz  [<a href='#Schumer1968'>Schumer1968</a>].
In the example, the algorithm runs for a fixed number of iterations and returns the best candidate solution discovered.
The example problem is an instance of a continuous function optimization that seeks $\min f(x)$ where $f=\sum_{i=1}^n x_{i}^2$, $-5.0 < x_i < 5.0$ and $n=2$. The optimal solution for this basin function is $(v_0,\ldots,v_{n-1})=0.0$.
</p>
<pre class='prettyprint lang-rb'>
def objective_function(vector)
  return vector.inject(0) {|sum, x| sum +  (x ** 2.0)}
end

def rand_in_bounds(min, max)
  return min + ((max-min) * rand())
end

def random_vector(minmax)
  return Array.new(minmax.size) do |i|
    rand_in_bounds(minmax[i][0], minmax[i][1])
  end
end

def take_step(minmax, current, step_size)
  position = Array.new(current.size)
  position.size.times do |i|
    min = [minmax[i][0], current[i]-step_size].max
    max = [minmax[i][1], current[i]+step_size].min
    position[i] = rand_in_bounds(min, max)
  end
  return position
end

def large_step_size(iter, step_size, s_factor, l_factor, iter_mult)
  return step_size * l_factor if iter&gt;0 and iter.modulo(iter_mult) == 0
  return step_size * s_factor
end

def take_steps(bounds, current, step_size, big_stepsize)
  step, big_step = {}, {}
  step[:vector] = take_step(bounds, current[:vector], step_size)
  step[:cost] = objective_function(step[:vector])
  big_step[:vector] = take_step(bounds,current[:vector],big_stepsize)
  big_step[:cost] = objective_function(big_step[:vector])
  return step, big_step
end

def search(max_iter, bounds, init_factor, s_factor, l_factor, iter_mult, max_no_impr)
  step_size = (bounds[0][1]-bounds[0][0]) * init_factor
  current, count = {}, 0
  current[:vector] = random_vector(bounds)
  current[:cost] = objective_function(current[:vector])
  max_iter.times do |iter|
    big_stepsize = large_step_size(iter, step_size, s_factor, l_factor, iter_mult)
    step, big_step = take_steps(bounds, current, step_size, big_stepsize)
    if step[:cost] &lt;= current[:cost] or big_step[:cost] &lt;= current[:cost]
      if big_step[:cost] &lt;= step[:cost]
        step_size, current = big_stepsize, big_step
      else
        current = step
      end
      count = 0
    else
      count += 1
      count, stepSize = 0, (step_size/s_factor) if count &gt;= max_no_impr
    end
    puts " &gt; iteration #{(iter+1)}, best=#{current[:cost]}"
  end
  return current
end

if __FILE__ == $0
  # problem configuration
  problem_size = 2
  bounds = Array.new(problem_size) {|i| [-5, +5]}
  # algorithm configuration
  max_iter = 1000
  init_factor = 0.05
  s_factor = 1.3
  l_factor = 3.0
  iter_mult = 10
  max_no_impr = 30
  # execute the algorithm
  best = search(max_iter, bounds, init_factor, s_factor, l_factor, iter_mult, max_no_impr)
  puts "Done. Best Solution: c=#{best[:cost]}, v=#{best[:vector].inspect}"
end
</pre>
<div class='download_src'><a href='adaptive_random_search.rb'>Download Source</a></div>
<div class='caption'>Adaptive Random Search in Ruby</div>


<a id='references'><h2>References</h2></a>
<a id='primary_sources'><h3>Primary Sources</h3></a>
<p>
Many works in the 1960s and 1970s experimented with variable step sizes for Random Search methods.
Schummer and Steiglitz are commonly credited the adaptive step size procedure, which they called 'Adaptive Step-Size Random Search'  [<a href='#Schumer1968'>Schumer1968</a>]. Their approach only modifies the step size based on an approximation of the optimal step size required to reach the global optima.
Kregting and White review adaptive random search methods and propose an approach called 'Adaptive Directional Random Search' that modifies both the algorithms step size and direction in response to the cost function  [<a href='#Kregting1971'>Kregting1971</a>].
</p>


<a id='learn_more'><h3>Learn More</h3></a>
<p>
White reviews extensions to Rastrigin's 'Creeping Random Search'  [<a href='#Rastrigin1963'>Rastrigin1963</a>] (fixed step size) that use probabilistic step sizes drawn stochastically from uniform and probabilistic distributions  [<a href='#White1971'>White1971</a>]. White also reviews works that propose dynamic control strategies for the step size, such as Karnopp  [<a href='#Karnopp1963'>Karnopp1963</a>] who proposes increases and decreases to the step size based on performance over very small numbers of trials.
Schrack and Choit review random search methods that modify their step size in order to approximate optimal moves while searching, including the property of reversal  [<a href='#Schrack1976'>Schrack1976</a>].
Masri et al. describe an adaptive random search strategy that alternates between periods of fixed and variable step sizes  [<a href='#Masri1980'>Masri1980</a>].
</p>




<h2>Bibliography</h2>
<table>
 <tr valign="top">
 <td><a id='Karnopp1963'>[Karnopp1963]</a></td>
 <td>D. C. Karnopp, "<a href="http://scholar.google.com.au/scholar?q=Random+search+techniques+for+optimization+problems">Random search techniques for optimization problems</a>", Automatica, 1963.</td>
 </tr>
 <tr valign="top">
 <td><a id='Kregting1971'>[Kregting1971]</a></td>
 <td>J. Kregting and R. C. White, "<a href="http://scholar.google.com.au/scholar?q=Adaptive+random+search">Adaptive random search</a>", Technical Report TH-Report 71-E-24, Eindhoven University of Technology, Eindhoven, Netherlands, 1971.</td>
 </tr>
 <tr valign="top">
 <td><a id='Masri1980'>[Masri1980]</a></td>
 <td>S. F. Masri and G. A. Bekey and F. B. Safford, "<a href="http://scholar.google.com.au/scholar?q=Global+Optimization+Algorithm+Using+Adaptive+Random+Search">Global Optimization Algorithm Using Adaptive Random Search</a>", Applied Mathematics and Computation, 1980.</td>
 </tr>
 <tr valign="top">
 <td><a id='Rastrigin1963'>[Rastrigin1963]</a></td>
 <td>L. A. Rastrigin, "<a href="http://scholar.google.com.au/scholar?q=The+Convergence+of+the+Random+Search+Method+in+the+Extremal+Control++of+a+Many+Parameter+System">The Convergence of the Random Search Method in the Extremal Control 	of a Many Parameter System</a>", Automation and Remote Control, 1963.</td>
 </tr>
 <tr valign="top">
 <td><a id='Schrack1976'>[Schrack1976]</a></td>
 <td>G. Schrack and M. Choit, "<a href="http://scholar.google.com.au/scholar?q=Optimized+relative+step+size+random+searches">Optimized relative step size random searches</a>", Mathematical Programming, 1976.</td>
 </tr>
 <tr valign="top">
 <td><a id='Schumer1968'>[Schumer1968]</a></td>
 <td>M. Schumer and K. Steiglitz, "<a href="http://scholar.google.com.au/scholar?q=Adaptive+step+size+random+search">Adaptive step size random search</a>", IEEE Transactions on Automatic Control, 1968.</td>
 </tr>
 <tr valign="top">
 <td><a id='White1971'>[White1971]</a></td>
 <td>R. C. White, "<a href="http://scholar.google.com.au/scholar?q=A+survey+of+random+methods+for+parameter+optimization">A survey of random methods for parameter optimization</a>", Simulation, 1971.</td>
 </tr>
</table>


  </body>
</html>  
