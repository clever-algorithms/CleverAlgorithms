<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
    <title>Clever Algorithms</title>
    <link rel="stylesheet" href="main.css" type="text/css" />
  </head>
  <body>
    
<a id='guided_local_search'><h1>Guided Local Search</h1></a>
<p>
<em>Guided Local Search, GLS.</em>
</p>

<a id='taxonomy'><h2>Taxonomy</h2></a>
<p>
The Guided Local Search algorithm is a Metaheuristic and a Global Optimization algorithm that makes use of an embedded Local Search algorithm.
It is an extension to Local Search algorithms such as Hill Climbing and is similar in strategy to the Tabu Search algorithm and the Iterated Local Search algorithm.
</p>


<a id='strategy'><h2>Strategy</h2></a>
<p>
The strategy for the Guided Local Search algorithm is to use penalties to encourage a Local Search technique to escape local optima and discover the global optima.
A Local Search algorithm is run until it gets stuck in a local optima. The features from the local optima are evaluated and penalized, the results of which are used in an augmented cost function employed by the Local Search procedure. The Local Search is repeated a number of times using the last local optima discovered and the augmented cost function that guides exploration away from solutions with features present in  discovered local optima.
</p>


<a id='procedure'><h2>Procedure</h2></a>
<p>
Algorithm (below) provides a pseudocode listing of the Guided Local Search algorithm for minimization.
The Local Search algorithm used by the Guided Local Search algorithm uses an augmented cost function in the form  $h(s)=g(s)+\lambda\cdot\sum_{i=1}^{M}f_i$, where
$h(s)$ is the augmented cost function, $g(s)$ is the problem cost function,$\lambda$ is the 'regularization parameter' (a coefficient for scaling the penalties), $s$ is a locally optimal solution of $M$ features, and $f_i$ is the $i$'th feature in locally optimal solution. The augmented cost function is only used by the local search procedure, the Guided Local Search algorithm uses the problem specific cost function without augmentation.
</p>
<p>
Penalties are only updated for those features in a locally optimal solution that maximize utility, updated by adding 1 to the penalty for the future (a counter).
The utility for a feature is calculated as $U_{feature}=\frac{C_{feature}}{1+P_{feature}}$, where $U_{feature}$ is the utility for penalizing a feature (maximizing), $C_{feature}$ is the cost of the feature, and $P_{feature}$ is the current penalty for the feature.
</p>
<div class='pseudocode'>
<strong><strong><code>Input</code></strong></strong>: 
$Iter_{max}$, $\lambda$
<br />
<strong><strong><code>Output</code></strong></strong>: 
$S_{best}$
<br />
$f_{penalties}$ $\leftarrow \emptyset$<br />
$S_{best}$ $\leftarrow$ <code>RandomSolution</code>()<br />
<strong><code>For</code></strong> ($Iter_{i}$ $\in$ $Iter_{max}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$S_{curr}$ $\leftarrow$ <code>LocalSearch</code>($S_{best}$, $\lambda$, $f_{penalties}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$f_{utilities}$ $\leftarrow$ <code>CalculateFeatureUtilities</code>($S_{curr}$, $f_{penalties}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;$f_{penalties}$ $\leftarrow$ <code>UpdateFeaturePenalties</code>($S_{curr}$, $f_{penalties}$, $f_{utilities}$)<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>If</code></strong> (<code>Cost</code>($S_{curr}$) $\leq$ <code>Cost</code>($S_{best}$))<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$S_{best}$ $\leftarrow$ $S_{curr}$<br />
&nbsp;&nbsp;&nbsp;&nbsp;<strong><code>End</code></strong><br />
<strong><code>End</code></strong><br />
<strong><code>Return</code></strong> ($S_{best}$)<br />
</div>
<div class='caption'>Pseudocode for Guided Local Search.</div>



<a id='heuristics'><h2>Heuristics</h2></a>
<ul>
<li> The Guided Local Search procedure is independent of the Local Search procedure embedded within it. A suitable domain-specific search procedure should be identified and employed.</li>
<li> The Guided Local Search procedure may need to be executed for thousands to hundreds-of-thousands of iterations, each iteration of which assumes a run of a Local Search algorithm to convergence.</li>
<li> The algorithm was designed for discrete optimization problems where a solution is comprised of independently assessable 'features' such as Combinatorial Optimization, although it has been applied to continuous function optimization modeled as binary strings.</li>
<li> The $\lambda$ parameter is a scaling factor for feature penalization that must be in the same proportion to the candidate solution costs from the specific problem instance to which the algorithm is being applied. As such, the value for $\lambda$ must be meaningful when used within the augmented cost function (such as when it is added to a candidate solution cost in minimization and subtracted from a cost in the case of a maximization problem).</li>
</ul>


<a id='code_listing'><h2>Code Listing</h2></a>
<p>
Listing (below) provides an example of the Guided Local Search algorithm implemented in the Ruby Programming Language.
The algorithm is applied to the Berlin52 instance of the Traveling Salesman Problem (TSP), taken from the TSPLIB. The problem seeks a permutation of the order to visit cities (called a tour) that minimizes the total distance traveled. The optimal tour distance for Berlin52 instance is 7542 units.
</p>
<p>
The implementation of the algorithm for the TSP was based on the configuration specified by Voudouris in  [<a href='#Voudouris1997'>Voudouris1997</a>].
A TSP-specific local search algorithm is used called 2-opt that selects two points in a permutation and reconnects the tour, potentially untwisting the tour at the selected points. The stopping condition for 2-opt was configured to be a fixed number of non-improving moves.
</p>
<p>
The equation for setting $\lambda$ for TSP instances is $\lambda = \alpha\cdot\frac{cost(optima)}{N}$, where $N$ is the number of cities, $cost(optima)$ is the cost of a local optimum found by a local search, and $\alpha\in (0,1]$ (around 0.3 for TSP and 2-opt). The cost of a local optima was fixed to the approximated value of 15000 for the Berlin52 instance.
The utility function for features (edges) in the TSP is $U_{edge}=\frac{D_{edge}}{1+P_{edge}}$, where $U_{edge}$ is the utility for penalizing an edge (maximizing), $D_{edge}$ is the cost of the edge (distance between cities) and $P_{edge}$ is the current penalty for the edge.
</p>
<pre class='prettyprint lang-rb'>
def euc_2d(c1, c2)
  Math.sqrt((c1[0] - c2[0])**2.0 + (c1[1] - c2[1])**2.0).round
end

def random_permutation(cities)
  perm = Array.new(cities.size){|i| i}
  perm.each_index do |i|
    r = rand(perm.size-i) + i
    perm[r], perm[i] = perm[i], perm[r]
  end
  return perm
end

def stochastic_two_opt(permutation)
  perm = Array.new(permutation)
  c1, c2 = rand(perm.size), rand(perm.size)
  exclude = [c1]
  exclude &lt;&lt; ((c1==0) ? perm.size-1 : c1-1)
  exclude &lt;&lt; ((c1==perm.size-1) ? 0 : c1+1)
  c2 = rand(perm.size) while exclude.include?(c2)
  c1, c2 = c2, c1 if c2 &lt; c1
  perm[c1...c2] = perm[c1...c2].reverse
  return perm
end

def augmented_cost(permutation, penalties, cities, lambda)
  distance, augmented = 0, 0
  permutation.each_with_index do |c1, i|
    c2 = (i==permutation.size-1) ? permutation[0] : permutation[i+1]
    c1, c2 = c2, c1 if c2 &lt; c1
    d = euc_2d(cities[c1], cities[c2])
    distance += d
    augmented += d + (lambda * (penalties[c1][c2]))
  end
  return [distance, augmented]
end

def cost(cand, penalties, cities, lambda)
  cost, acost = augmented_cost(cand[:vector], penalties, cities, lambda)
  cand[:cost], cand[:aug_cost] = cost, acost
end

def local_search(current, cities, penalties, max_no_improv, lambda)
  cost(current, penalties, cities, lambda)
  count = 0
  begin
    candidate = {:vector=&gt; stochastic_two_opt(current[:vector])}
    cost(candidate, penalties, cities, lambda)
    count = (candidate[:aug_cost] &lt; current[:aug_cost]) ? 0 : count+1
    current = candidate if candidate[:aug_cost] &lt; current[:aug_cost]
  end until count &gt;= max_no_improv
  return current
end

def calculate_feature_utilities(penal, cities, permutation)
  utilities = Array.new(permutation.size,0)
  permutation.each_with_index do |c1, i|
    c2 = (i==permutation.size-1) ? permutation[0] : permutation[i+1]
    c1, c2 = c2, c1 if c2 &lt; c1
    utilities[i] = euc_2d(cities[c1], cities[c2]) / (1.0 + penal[c1][c2])
  end
  return utilities
end

def update_penalties!(penalties, cities, permutation, utilities)
  max = utilities.max()
  permutation.each_with_index do |c1, i|
    c2 = (i==permutation.size-1) ? permutation[0] : permutation[i+1]
    c1, c2 = c2, c1 if c2 &lt; c1
    penalties[c1][c2] += 1 if utilities[i] == max
  end
  return penalties
end

def search(max_iterations, cities, max_no_improv, lambda)
  current = {:vector=&gt;random_permutation(cities)}
  best = nil
  penalties = Array.new(cities.size){ Array.new(cities.size, 0) }
  max_iterations.times do |iter|
    current=local_search(current, cities, penalties, max_no_improv, lambda)
    utilities=calculate_feature_utilities(penalties,cities,current[:vector])
    update_penalties!(penalties, cities, current[:vector], utilities)
    best = current if best.nil? or current[:cost] &lt; best[:cost]
    puts " &gt; iter=#{(iter+1)}, best=#{best[:cost]}, aug=#{best[:aug_cost]}"
  end
  return best
end

if __FILE__ == $0
  # problem configuration
  berlin52 = [[565,575],[25,185],[345,750],[945,685],[845,655],
   [880,660],[25,230],[525,1000],[580,1175],[650,1130],[1605,620],
   [1220,580],[1465,200],[1530,5],[845,680],[725,370],[145,665],
   [415,635],[510,875],[560,365],[300,465],[520,585],[480,415],
   [835,625],[975,580],[1215,245],[1320,315],[1250,400],[660,180],
   [410,250],[420,555],[575,665],[1150,1160],[700,580],[685,595],
   [685,610],[770,610],[795,645],[720,635],[760,650],[475,960],
   [95,260],[875,920],[700,500],[555,815],[830,485],[1170,65],
   [830,610],[605,625],[595,360],[1340,725],[1740,245]]
  # algorithm configuration
  max_iterations = 150
  max_no_improv = 20
  alpha = 0.3
  local_search_optima = 12000.0
  lambda = alpha * (local_search_optima/berlin52.size.to_f)
  # execute the algorithm
  best = search(max_iterations, berlin52, max_no_improv, lambda)
  puts "Done. Best Solution: c=#{best[:cost]}, v=#{best[:vector].inspect}"
end
</pre>
<div class='download_src'><a href='guided_local_search.rb'>Download Source</a></div>
<div class='caption'>Guided Local Search in Ruby</div>


<a id='references'><h2>References</h2></a>

<a id='primary_sources'><h3>Primary Sources</h3></a>
<p>
Guided Local Search emerged from an approach called GENET, which is a connectionist approach to constraint satisfaction  [<a href='#Wang1991'>Wang1991</a>] [<a href='#Tsang1992'>Tsang1992</a>].
Guided Local Search was presented by Voudouris and Tsang in a series of technical reports (that were later published) that described the technique and provided example applications of it to constraint satisfaction  [<a href='#Voudouris1994'>Voudouris1994</a>], combinatorial optimization  [<a href='#Voudouris1995b'>Voudouris1995b</a>] [<a href='#Voudouris1995'>Voudouris1995</a>], and function optimization  [<a href='#Voudouris1995a'>Voudouris1995a</a>].
The seminal work on the technique was Voudouris' PhD dissertation  [<a href='#Voudouris1997'>Voudouris1997</a>].
</p>


<a id='learn_more'><h3>Learn More</h3></a>
<p>
Voudouris and Tsang provide a high-level introduction to the technique  [<a href='#Voudouris1998'>Voudouris1998</a>], and a contemporary summary of the approach in Glover and Kochenberger's 'Handbook of metaheuristics'  [<a href='#Glover2003a'>Glover2003a</a>] that includes a review of the technique, application areas, and demonstration applications on a diverse set of problem instances.
Mills et al. elaborated on the approach, devising an 'Extended Guided Local Search' (EGLS) technique that added 'aspiration criteria' and random moves to the procedure  [<a href='#Mills2003'>Mills2003</a>], work which culminated in Mills' PhD dissertation  [<a href='#Mills2002'>Mills2002</a>].
Lau and Tsang further extended the approach by integrating it with a Genetic Algorithm, called the 'Guided Genetic Algorithm' (GGA)  [<a href='#Lau1998'>Lau1998</a>], that also culminated in a PhD dissertation by Lau  [<a href='#Lau1999'>Lau1999</a>].
</p>




<h2>Bibliography</h2>
<table>
 <tr valign="top">
 <td><a id='Glover2003a'>[Glover2003a]</a></td>
 <td>C. Voudouris and E. P. K. Tsang, "<a href="http://scholar.google.com.au/scholar?q=7:+Guided+Local+Search">7: Guided Local Search</a>", in Handbook of Metaheuristics, pages 185-218, Springer, 2003.</td>
 </tr>
 <tr valign="top">
 <td><a id='Lau1998'>[Lau1998]</a></td>
 <td>T. L. Lau and E. P. K. Tsang, "<a href="http://scholar.google.com.au/scholar?q=The+guided+genetic+algorithm+and+its+application+to+the+general+assignment++problems">The guided genetic algorithm and its application to the general assignment 	problems</a>", in IEEE 10th International Conference on Tools with Artificial Intelligence 	(ICTAI'98), 1998.</td>
 </tr>
 <tr valign="top">
 <td><a id='Lau1999'>[Lau1999]</a></td>
 <td>L. T. Lau, "<a href="http://scholar.google.com.au/scholar?q=Guided+Genetic+Algorithm">Guided Genetic Algorithm</a>", [PhD Thesis] Department of Computer Science, University of Essex, 1999.</td>
 </tr>
 <tr valign="top">
 <td><a id='Mills2002'>[Mills2002]</a></td>
 <td>P. Mills, "<a href="http://scholar.google.com.au/scholar?q=Extensions+to+Guided+Local+Search">Extensions to Guided Local Search</a>", [PhD Thesis] Department of Computer Science, University of Essex, 2002.</td>
 </tr>
 <tr valign="top">
 <td><a id='Mills2003'>[Mills2003]</a></td>
 <td>P. Mills and E. Tsang and J. Ford, "<a href="http://scholar.google.com.au/scholar?q=Applying+an+Extended+Guided+Local+Search+on+the+Quadratic+Assignment++Problem">Applying an Extended Guided Local Search on the Quadratic Assignment 	Problem</a>", Annals of Operations Research, 2003.</td>
 </tr>
 <tr valign="top">
 <td><a id='Tsang1992'>[Tsang1992]</a></td>
 <td>E. P. K. Tsang and C. J. Wang, "<a href="http://scholar.google.com.au/scholar?q=A+Generic+Neural+Network+Approach+For+Constraint+Satisfaction+Problems">A Generic Neural Network Approach For Constraint Satisfaction Problems</a>", in Neural network applications, 1992.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1994'>[Voudouris1994]</a></td>
 <td>C. Voudouris and E. Tsang, "<a href="http://scholar.google.com.au/scholar?q=The+Tunneling+Algorithm+for+Partial+CSPs+and+Combinatorial+Optimization++Problems">The Tunneling Algorithm for Partial CSPs and Combinatorial Optimization 	Problems</a>", Technical Report CSM-213, Department of Computer Science, University of Essex, Colchester, 	C04 3SQ, UK, 1994.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1995'>[Voudouris1995]</a></td>
 <td>C. Voudouris and E. Tsang, "<a href="http://scholar.google.com.au/scholar?q=Guided+Local+Search">Guided Local Search</a>", Technical Report CSM-247, Department of Computer Science, University of Essex, Colchester, 	C04 3SQ, UK, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1995a'>[Voudouris1995a]</a></td>
 <td>C. Voudouris and E. Tsang, "<a href="http://scholar.google.com.au/scholar?q=Function+Optimization+using+Guided+Local+Search">Function Optimization using Guided Local Search</a>", Technical Report CSM-249, Department of Computer Science University of Essex Colchester, CO4 	3SQ, UK, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1995b'>[Voudouris1995b]</a></td>
 <td>E. Tsang and C. Voudouris, "<a href="http://scholar.google.com.au/scholar?q=Fast+Local+Search+and+Guided+Local+Search+and+Their+Application+to++British+Telecom's+Workforce+Scheduling+Problem">Fast Local Search and Guided Local Search and Their Application to 	British Telecom's Workforce Scheduling Problem</a>", Technical Report CSM-246, Department of Computer Science University of Essex Colchester CO4 	3SQ, 1995.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1997'>[Voudouris1997]</a></td>
 <td>C. Voudouris, "<a href="http://scholar.google.com.au/scholar?q=Guided+local+search+for+combinatorial+optimisation+problems">Guided local search for combinatorial optimisation problems</a>", [PhD Thesis] Department of Computer Science, University of Essex, Colchester, 	UK, 1997.</td>
 </tr>
 <tr valign="top">
 <td><a id='Voudouris1998'>[Voudouris1998]</a></td>
 <td>C. Voudouris and E. P. K. Tsang, "<a href="http://scholar.google.com.au/scholar?q=Guided+local+search+joins+the+elite+in+discrete+optimisation">Guided local search joins the elite in discrete optimisation</a>", in Proceedings, DIMACS Workshop on Constraint Programming and Large 	Scale Discrete Optimisation, 1998.</td>
 </tr>
 <tr valign="top">
 <td><a id='Wang1991'>[Wang1991]</a></td>
 <td>C. J. Wang and E. P. K. Tsang, "<a href="http://scholar.google.com.au/scholar?q=Solving+constraint+satisfaction+problems+using+neural+networks">Solving constraint satisfaction problems using neural networks</a>", in Proceedings Second International Conference on Artificial Neural Networks, 1991.</td>
 </tr>
</table>


  </body>
</html>  
