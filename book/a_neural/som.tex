% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Self-Organizing Map} 
\label{sec:som}
\index{Self-Organizing Map}
\index{Kohonen Network}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Self-Organizing Map, SOM, Self-Organizing Feature Map, SOFM, Kohonen Map, Kohonen Network.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
The Self-Organizing Map algorithm belongs to the field of Artificial Neural Networks and Neural Computation. More broadly it belongs to the field of Computational Intelligence.  
% What are the closely related approaches to a technique?
The Self-Organizing Map is an unsupervised neural network that uses a competitive (winner-take-all) learning strategy.
It is related to other unsupervised neural networks such as the Adaptive Resonance Theory (ART) method.
It is related to other competitive learning neural networks such as the the Neural Gas Algorithm, and the Learning Vector Quantization algorithm (Section~\ref{sec:lvq}), which is a similar algorithm for classification without connections between the neurons. 
Additionally, SOM is a baseline technique that has inspired many variations and extensions, not limited to the Adaptive-Subspace Self-Organizing Map (ASSOM).

% Inspiration: Motivating system
% The inspiration describes the specific system or process that provoked the inception of the algorithm. The inspiring system may non-exclusively be natural, biological, physical, or social. The description of the inspiring system may include relevant domain specific theory, observation, nomenclature, and most important must include those salient attributes of the system that are somehow abstractly or conceptually manifest in the technique. The inspiration is described textually with citations and may include diagrams to highlight features and relationships within the inspiring system.
% Optional
\subsection{Inspiration}
% What is the system or process that motivated the development of a technique?
% Which features of the motivating system are relevant to a technique?
The Self-Organizing Map is inspired by postulated feature maps of neurons in the brain comprised of feature-sensitive cells that provide ordered projections between neuronal layers, such as those that may exist in the retina and cochlea. For example, there are acoustic feature maps that respond to sounds to which an animal is most frequently exposed, and tonotopic maps that may be responsible for the order preservation of acoustic resonances.

% Metaphor: Explanation via analogy
% The metaphor is a description of the technique in the context of the inspiring system or a different suitable system. The features of the technique are made apparent through an analogous description of the features of the inspiring system. The explanation through analogy is not expected to be literal scientific truth, rather the method is used as an allegorical communication tool. The inspiring system is not explicitly described, this is the role of the ‘inspiration’ element, which represents a loose dependency for this element. The explanation is textual and uses the nomenclature of the metaphorical system.
% Optional
% \subsection{Metaphor}
% What is the explanation of a technique in the context of the inspiring system?
% What are the functionalities inferred for a technique from the analogous inspiring system?
% A textual description of the algorithm by analogy.

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The information processing objective of the algorithm is to optimally place a topology (grid or lattice) of codebook or prototype vectors in the domain of the observed input data samples.
% What is a techniques plan of action?
An initially random pool of vectors is prepared which are then exposed to training samples. A winner-take-all strategy is employed where the most similar vector to a given input pattern is selected, then the selected vector and neighbors of the selected vector are updated to closer resemble the input pattern. The repetition of this process results in the distribution of codebook vectors in the input space which approximate the underlying distribution of samples from the test dataset. The result is the mapping of the topology of codebook vectors to the underlying structure in the input samples which may be summarized or visualized to reveal topologically preserved features from the input space in a low-dimensional projection.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What are the data structures and representations used in a technique?
The Self-Organizing map is comprised of a collection of codebook vectors connected together in a topological arrangement, typically a one dimensional line or a two dimensional grid. The codebook vectors themselves represent prototypes (points) within the domain, whereas the topological structure imposes an ordering between the vectors during the training process. The result is a low dimensional projection or approximation of the problem domain which may be visualized, or from which clusters may be extracted.

% What is the computational recipe for a technique?
Algorithm~\ref{alg:train} provides a high-level pseudocode for preparing codebook vectors using the Self-Organizing Map method. 
Codebook vectors are initialized to small floating point values, or sampled from the domain. The Best Matching Unit (BMU) is the codebook vector from the pool that has the minimum distance to an input vector. A distance measure between input patterns must be defined. For real-valued vectors, this is commonly the Euclidean distance:

\begin{equation}
	dist(x,c) = \sum_{i=1}^{n} (x_i - c_i)^2
\end{equation}

where $n$ is the number of attributes, $x$ is the input vector and $c$ is a given codebook vector.

The neighbors of the BMU in the topological structure of the network are selected using a neighborhood size that is linearly decreased during the training of the network. The BMU and all selected neighbors are then adjusted toward the input vector using a learning rate that too is decreased linearly with the training cycles:

\begin{equation}
	c_i(t+1) = learn_{rate}(t) \times (c_i(t) - x_i)
\end{equation}

where $c_i(t)$ is the $i^{th}$ attribute of a codebook vector at time $t$, $learn_{rate}$ is the current learning rate, an $x_i$ is the $i^{th}$ attribute of a input vector.

The neighborhood is typically square (called bubble) where all neighborhood nodes are updated using the same learning rate for the iteration, or Gaussian where the learning rate is proportional to the neighborhood distance using a Gaussian distribution (neighbors further away from the BMU are updated less). 

\begin{algorithm}[ht]
	\SetLine

	% parametrs
	\SetKwData{InputPatterns}{InputPatterns}
	\SetKwData{MaxIterations}{$iterations_{max}$}
	\SetKwData{LearningRate}{$learn_{rate}^{init}$}	
	\SetKwData{NeighborhoodSize}{$neighborhood_{size}^{init}$}
	\SetKwData{GridWidth}{$Grid_{width}$}
	\SetKwData{GridHeight}{$Grid_{height}$}
	% data
	\SetKwData{CodebookVectors}{CodebookVectors}
	\SetKwData{Neighborhood}{Neighborhood}
	\SetKwData{Lrate}{$learn_{rate}^{i}$}	
	\SetKwData{Nsize}{$neighborhood_{size}^{i}$}
	\SetKwData{Pattern}{$Pattern_i$}	
	\SetKwData{Bmu}{$Bmu_i$}
	\SetKwData{Vector}{$Vector_i$}
	\SetKwData{PatternAttribute}{$Pattern_{i}^{attribute}$}
	\SetKwData{VectorAttribute}{$Vector_{i}^{attribute}$}
	
	% functions
	\SetKwFunction{InitializeCodebookVectors}{InitializeCodebookVectors}
	\SetKwFunction{CalculateLearningRate}{CalculateLearningRate}
	\SetKwFunction{CalculateNeighborhoodSize}{CalculateNeighborhoodSize}
	\SetKwFunction{SelectInputPattern}{SelectInputPattern}
	\SetKwFunction{SelectBestMatchingUnit}{SelectBestMatchingUnit}
	\SetKwFunction{SelectNeighbors}{SelectNeighbors}
	
	% I/O
	\KwIn{\InputPatterns, \MaxIterations, \LearningRate, \NeighborhoodSize, \GridWidth, \GridHeight}		
	\KwOut{\CodebookVectors}
  
	% Algorithm
	\CodebookVectors $\leftarrow$ \InitializeCodebookVectors{\GridWidth, \GridHeight, \InputPatterns}\;
	% loop
	\For{$i=1$ \KwTo \MaxIterations} {
		\Lrate $\leftarrow$ \CalculateLearningRate{$i$, \LearningRate}\;
		\Nsize $\leftarrow$ \CalculateNeighborhoodSize{$i$, \NeighborhoodSize}\;
		\Pattern $\leftarrow$ \SelectInputPattern{\InputPatterns}\;
		\Bmu $\leftarrow$ \SelectBestMatchingUnit{\Pattern, \CodebookVectors}\;
		\Neighborhood $\leftarrow$ \Bmu\;
		\Neighborhood $\leftarrow$ \SelectNeighbors{\Bmu, \CodebookVectors, \Nsize}\;
		\ForEach{\Vector $\in $\Neighborhood} {
			\ForEach{\VectorAttribute $\in$ \Vector}{
				\VectorAttribute $\leftarrow$ \VectorAttribute $+$ \Lrate $\times$ (\PatternAttribute $-$ \VectorAttribute)
			}
		}
	}
	\Return{\CodebookVectors}\;
	% end
	\caption{Pseudocode for the SOM.}
	\label{alg:train}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item The Self-Organizing Map was designed for unsupervised learning problems such as feature extraction, visualization and clustering. Some extensions of the approach can label the prepared codebook vectors which can be used for classification.
	\item SOM is non-parametric, meaning that it does not rely on assumptions about that structure of the function that it is approximating.
	\item Real-values in input vectors should be normalized such that $x \in [0,1)$. 
	\item Euclidean distance is commonly used to measure the distance between real-valued vectors, although other distance measures may be used (such as dot product), and data specific distance measures may be required for non-scalar attributes.
	\item There should be sufficient training iterations to expose all the training data to the model multiple times.
	\item The more complex the class distribution, the more codebook vectors that will be required, some problems may need thousands.
	\item Multiple passes of the SOM training algorithm are suggested for more robust usage, where the first pass has a large learning rate to prepare the codebook vectors and the second pass has a low learning rate and runs for a long time (perhaps 10-times more iterations).
	\item The SOM can be visualized by calculating a Unified Distance Matrix (U-Matrix) shows highlights the relationships between the nodes in the chosen topology. A Principle Component Analysis (PCA) or Sammon's Mapping can be used to visualize just the nodes of the network without their inter-relationships.
	\item A rectangular 2D grid topology is typically used for a SOM, although toroidal and sphere topologies can be used. Hexagonal grids have demonstrated better results on some problems and grids with higher dimensions have been investigated. 
	\item The neuron positions can be updated incrementally or in a batch model (each epoch of being exposed to all training samples). Batch-mode training is generally expected to result in a more stable network.
	\item The learning rate and neighborhood size parameters typically decrease linearly with the training iterations, although non-linear functions may be used.
\end{itemize}

% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{som} provides an example of the Self-Organizing Map algorithm implemented in the Ruby Programming Language. 
% problem
The problem is a feature detection problem, where the network is expected to learn a predefined shape based on being exposed to samples in the domain. The domain is two-dimensional $x,y \in [0,1]$, where a shape is pre-defined as a square in the middle of the domain $x,y \in [0.3,0.6]$. The system is initialized to vectors within the domain although is only exposed to samples within the pre-defined shape during training. The expectation is that the system will model the shape based on the observed samples.

% algorithm
The algorithm is an implementation of the basic Self-Organizing Map algorithm based on the description in Chapter~3 of the seminal book on the technique \cite{Kohonen1995}. The implementation is configured with a $4 \times 5$ grid of nodes, the Euclidean distance measure is used to determine the BMU and neighbors, a Bubble neighborhood function is used. Error rates are presented to the console, and the codebook vectors themselves are described before and after training. The learning process is incremental rather than batch, for simplicity. 

An extension to this implementation would be to visualize the resulting network structure in the domain - shrinking from a mesh that covers the whole domain, down to a mesh that only covers the pre-defined shape within the domain.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Self-Organizing Map in Ruby, label=som]{../src/algorithms/neural/som.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% seminal
The Self-Organizing Map was proposed by Kohonen in 1982 in a study that included the mathematical basis for the approach, summary of related physiology, and simulation on demonstration problem domains using one and two dimensional topological structures \cite{Kohonen1982}.
% early
This work was tightly related two other papers published at close to the same time on topological maps and self-organization \cite{Kohonen1981, Kohonen1982b}.

% 
% Learn More
% 
\subsubsection{Learn More}
% reviews
Kohonen provides a detailed introduction and summary of the Self-Organizing Map in a journal article \cite{Kohonen1990a}.
Kohonen et al.\ provide a practical presentation of the algorithm and heuristics for configuration in the technical report written to accompany the released SOM-PAK implementation of the algorithm for academic research \cite{Kohonen1996a}.
% books
The seminal book on the technique is ``Self-Organizing Maps'' by Kohonen, which includes chapters dedicated to the description of the basic approach, physiological interpretations of the algorithm, variations, and summaries of application areas \cite{Kohonen1995}.


