% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Learning Vector Quantization} 
\label{sec:lvq}
\index{Learning Vector Quantization}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Learning Vector Quantization, LVQ.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
The Learning Vector Quantization algorithm belongs to the field of Artificial Neural Networks and Neural Computation. More broadly to the field of Computational Intelligence.  
% What are the closely related approaches to a technique?
The Learning Vector Quantization algorithm is a supervised neural network that uses a competitive (winner-take-all) learning strategy.
It is related to other supervised neural networks such as the Perceptron (Section~\ref{sec:perceptron}) and the Back-propagation algorithm (Section~\ref{sec:backpropagation}).
It is related to other competitive learning neural networks such as the the Self-Organizing Map algorithm (Section~\ref{sec:som}) that is a similar algorithm for unsupervised learning with the addition of connections between the neurons.
Additionally, LVQ is a baseline technique that was defined with a few variants LVQ1, LVQ2, LVQ2.1, LVQ3, OLVQ1, and OLVQ3 as well as many third-party extensions and refinements too numerous to list.

% Inspiration: Motivating system
% The inspiration describes the specific system or process that provoked the inception of the algorithm. The inspiring system may non-exclusively be natural, biological, physical, or social. The description of the inspiring system may include relevant domain specific theory, observation, nomenclature, and most important must include those salient attributes of the system that are somehow abstractly or conceptually manifest in the technique. The inspiration is described textually with citations and may include diagrams to highlight features and relationships within the inspiring system.
% Optional
\subsection{Inspiration}
% What is the system or process that motivated the development of a technique?
% Which features of the motivating system are relevant to a technique?
The Learning Vector Quantization algorithm is related to the Self-Organizing Map which is in turn inspired by the self-organizing capabilities of neurons in the visual cortex. 

% Metaphor: Explanation via analogy
% The metaphor is a description of the technique in the context of the inspiring system or a different suitable system. The features of the technique are made apparent through an analogous description of the features of the inspiring system. The explanation through analogy is not expected to be literal scientific truth, rather the method is used as an allegorical communication tool. The inspiring system is not explicitly described, this is the role of the ‘inspiration’ element, which represents a loose dependency for this element. The explanation is textual and uses the nomenclature of the metaphorical system.
% Optional
% \subsection{Metaphor}
% What is the explanation of a technique in the context of the inspiring system?
% What are the functionalities inferred for a technique from the analogous inspiring system?
% A textual description of the algorithm by analogy.

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The information processing objective of the algorithm is to prepare a set of codebook (or prototype) vectors in the domain of the observed input data samples and to use these vectors to classify unseen examples.
% What is a techniques plan of action?
An initially random pool of vectors is prepared which are then exposed to training samples. A winner-take-all strategy is employed where one or more of the most similar vectors to a given input pattern are selected and adjusted to be closer to the input vector, and in some cases, further away from the winner for runners up. The repetition of this process results in the distribution of codebook vectors in the input space which approximate the underlying distribution of samples from the test dataset.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What are the data structures and representations used in a technique?
Vector Quantization is a technique from signal processing where density functions are approximated with prototype vectors for applications such as compression. Learning Vector Quantization is similar in principle, although the prototype vectors are learned through a supervised winner-take-all method.

% What is the computational recipe for a technique?
Algorithm~\ref{alg:train} provides a high-level pseudocode for preparing codebook vectors using the Learning Vector Quantization method. 
Codebook vectors are initialized to small floating point values, or sampled from an available dataset. The Best Matching Unit (BMU) is the codebook vector from the pool that has the minimum distance to an input vector. A distance measure between input patterns must be defined. For real-valued vectors, this is commonly the Euclidean distance:

\begin{equation}
	dist(x,c) = \sum_{i=1}^{n} (x_i - c_i)^2
\end{equation}

where $n$ is the number of attributes, $x$ is the input vector and $c$ is a given codebook vector.

\begin{algorithm}[ht]
	\SetLine

	% data
	\SetKwData{ProblemSize}{ProblemSize}
	\SetKwData{MaxIterations}{$iterations_{max}$}
	\SetKwData{LearningRate}{$learn_{rate}$}
	\SetKwData{CodebookVectors}{CodebookVectors}
	\SetKwData{InputPatterns}{InputPatterns}
	\SetKwData{Pattern}{$Pattern_i$}
	\SetKwData{PatternAttribute}{$Pattern_{i}^{attribute}$}
	\SetKwData{PatternClass}{$Pattern_{i}^{class}$}
	\SetKwData{Output}{$Output_i$}
	\SetKwData{NumCodebookVectors}{$CodebookVectors_{num}$}
	\SetKwData{Bmu}{$Bmu_i$}
	\SetKwData{BmuClass}{$Bmu_{i}^{class}$}
	\SetKwData{BmuAttribute}{$Bmu_{i}^{attribute}$}
	
	% functions
	\SetKwFunction{InitializeCodebookVectors}{InitializeCodebookVectors}
	\SetKwFunction{SelectInputPattern}{SelectInputPattern}
	\SetKwFunction{SelectBestMatchingUnit}{SelectBestMatchingUnit}

	
	% I/O
	\KwIn{\ProblemSize, \InputPatterns, \MaxIterations, \NumCodebookVectors, \LearningRate}		
	\KwOut{\CodebookVectors}
  
	% Algorithm
	\CodebookVectors $\leftarrow$ \InitializeCodebookVectors{\NumCodebookVectors, \ProblemSize}\;
	% loop
	\For{$i=1$ \KwTo \MaxIterations} {
		\Pattern $\leftarrow$ \SelectInputPattern{\InputPatterns}\;
		\Bmu $\leftarrow$ \SelectBestMatchingUnit{\Pattern, \CodebookVectors}\;
		\ForEach{\BmuAttribute $\in$ \Bmu}{
			\eIf{\BmuClass $\equiv$ \PatternClass} {
				\BmuAttribute $\leftarrow$ \BmuAttribute $+$ \LearningRate $\times$ (\PatternAttribute $-$ \BmuAttribute)
			}{
				\BmuAttribute $\leftarrow$ \BmuAttribute $-$ \LearningRate $\times$ (\PatternAttribute $-$ \BmuAttribute)
			}
		}
	}
	\Return{\CodebookVectors}\;
	% end
	\caption{Pseudocode for LVQ1.}
	\label{alg:train}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item Learning Vector Quantization was designed for classification problems that have existing data sets that can be used to supervise the learning by the system. The algorithm does not support regression problems.
	\item LVQ is non-parametric, meaning that it does not rely on assumptions about that structure of the function that it is approximating.
	\item Real-values in input vectors should be normalized such that $x \in [0,1)$. 
	\item Euclidean distance is commonly used to measure the distance between real-valued vectors, although other distance measures may be used (such as dot product), and data specific distance measures may be required for non-scalar attributes.
	\item There should be sufficient training iterations to expose all the training data to the model multiple times.
	\item The learning rate is typically linearly decayed over the training period from an initial value to close to zero.
	\item The more complex the class distribution, the more codebook vectors that will be required, some problems may need thousands.
	\item Multiple passes of the LVQ training algorithm are suggested for more robust usage, where the first pass has a large learning rate to prepare the codebook vectors and the second pass has a low learning rate and runs for a long time (perhaps 10-times more iterations).
\end{itemize}

% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{lvq} provides an example of the Learning Vector Quantization algorithm implemented in the Ruby Programming Language. 
% problem
The problem is a contrived classification problem in a 2-dimensional domain $x\in[0,1], y\in[0,1]$ with two classes: `A' ($x\in[0,0.4999999], y\in[0,0.4999999]$) and `B' ($x\in[0.5,1], y\in[0.5,1]$).

% algorithm
The algorithm was implemented using the LVQ1 variant where the best matching codebook vector is located and moved toward the input vector if it is the same class, or away if the classes differ. A linear decay was used for the learning rate that was updated after each pattern was exposed to the model. The implementation can easily be extended to the other variants of the method.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Learning Vector Quantization in Ruby, label=lvq]{../src/algorithms/neural/lvq.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% seminal
The Learning Vector Quantization algorithm was described by Kohonen in 1988 \cite{Kohonen1988}, and was further described in the same year by Kohonen \cite{Kohonen1988a} and benchmarked by Kohonen, Barna, and Chrisley \cite{Kohonen1988b}.

% 
% Learn More
% 
\subsubsection{Learn More}
% reviews
Kohonen provides a detailed overview of the state of LVQ algorithms and variants (LVQ1, LVQ2, and LVQ2.1) \cite{Kohonen1990}. The technical report that comes with the LVQ\_PAK software (written by Kohonen and his students) provides both an excellent summary of the technique and its main variants, as well as summarizing the important considerations when applying the approach \cite{Kohonen1996}.
% books
The seminal book on Learning Vector Quantization and the Self-Organizing Map is ``Self-Organizing Maps'' by Kohonen, which includes a chapter (Chapter 6) dedicated to LVQ and its variants \cite{Kohonen1995}.


