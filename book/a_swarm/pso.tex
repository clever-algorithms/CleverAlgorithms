% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Particle Swarm Optimization} 
\label{sec:pso}
\index{Particle Swarm Optimization}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Particle Swarm Optimization, PSO.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
Particle Swarm Optimization belongs to the field of Swarm Intelligence and Collective Intelligence and is a sub-field of Computational Intelligence.
% What are the closely related approaches to a technique?
Particle Swarm Optimization is related to other Swarm Intelligence algorithms such as Ant Colony Optimization and it is a baseline algorithm for many variations, too numerous to list.

% Inspiration: Motivating system
% The inspiration describes the specific system or process that provoked the inception of the algorithm. The inspiring system may non-exclusively be natural, biological, physical, or social. The description of the inspiring system may include relevant domain specific theory, observation, nomenclature, and most important must include those salient attributes of the system that are somehow abstractly or conceptually manifest in the technique. The inspiration is described textually with citations and may include diagrams to highlight features and relationships within the inspiring system.
% Optional
\subsection{Inspiration}
% What is the system or process that motivated the development of a technique?
Particle Swarm Optimization is inspired by the social foraging behavior of some animals such as flocking behavior of birds and the schooling behavior of fish.
% Which features of the motivating system are relevant to a technique?

% Metaphor: Explanation via analogy
% The metaphor is a description of the technique in the context of the inspiring system or a different suitable system. The features of the technique are made apparent through an analogous description of the features of the inspiring system. The explanation through analogy is not expected to be literal scientific truth, rather the method is used as an allegorical communication tool. The inspiring system is not explicitly described, this is the role of the ‘inspiration’ element, which represents a loose dependency for this element. The explanation is textual and uses the nomenclature of the metaphorical system.
% Optional
\subsection{Metaphor}
% What is the explanation of a technique in the context of the inspiring system?
Particles in the swarm fly through an environment following the fitter members of the swarm and generally biasing their movement toward historically good areas of their environment.
% What are the functionalities inferred for a technique from the analogous inspiring system?

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The goal of the algorithm is to have all the particles locate the optima in a multi-dimensional hyper-volume.
% What is a techniques plan of action?
This is achieved by assigning initially random positions to all particles in the space and small initial random velocities. The algorithm is executed like a simulation, advancing the position of each particle in turn based on its velocity, the best known global position in the problem space and the best position known to a particle. The objective function is sampled after each position update. Over time, through a combination of exploration and exploitation of known good positions in the search space, the particles cluster or converge together around an optima, or several optima.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What are the data structures and representations used in a technique?
The Particle Swarm Optimization algorithm is comprised of a collection of particles that move around the search space influenced by their own best past location and the best past location of the whole swarm or a close neighbor. Each iteration a particle's velocity is updated using:

\begin{align*}
	v_{i}(t+1) = v_{i}(t) + 
	&\big( c_1 \times rand() \times (p_{i}^{best} - p_{i}(t)) \big) +  \\
	&\big( c_2 \times rand() \times (p_{gbest} - p_{i}(t)) \big)
\end{align*}

where $v_{i}(t+1)$ is the new velocity for the $i^{th}$ particle, $c_1$ and $c_2$ are the weighting coefficients for the personal best and global best positions respectively, $p_{i}(t)$ is the $i^{th}$ particle's position at time $t$, $p_{i}^{best}$ is the $i^{th}$ particle's best known position, and $p_{gbest}$ is the best position known to the swarm. The $rand()$ function generate a uniformly random variable $\in [0,1]$. Variants on this update equation consider best positions within a particles local neighborhood at time $t$.

A particle's position is updated using:

\begin{equation}
	p_{i}(t+1) = p_{i}(t) + v_{i}(t)
\end{equation}

% What is the computational recipe for a technique?
Algorithm~\ref{alg:pso} provides a pseudocode listing of the Particle Swarm Optimization algorithm for minimizing a cost function. 

\begin{algorithm}[ht]
	\SetLine  

	% data
	\SetKwData{GlobalBest}{$P_{g\_best}$}
	\SetKwData{ProblemSize}{ProblemSize}
	\SetKwData{Population}{Population}
	\SetKwData{PopulationSize}{$Population_{size}$}
	\SetKwData{Particle}{$P$}
	\SetKwData{CurrentBest}{$P_{p\_best}$}
	\SetKwData{CurrentPosition}{$P_{position}$}
	\SetKwData{CurrentVelocity}{$P_{velocity}$}
	% functions
	\SetKwFunction{Cost}{Cost}
	\SetKwFunction{StopCondition}{StopCondition}
	\SetKwFunction{RandomPosition}{RandomPosition}
	\SetKwFunction{RandomVelocity}{RandomVelocity}	
	\SetKwFunction{UpdatePosition}{UpdatePosition}
	\SetKwFunction{UpdateVelocity}{UpdateVelocity}
	
	% I/O
	\KwIn{\ProblemSize, \PopulationSize}		
	\KwOut{\GlobalBest}
  % Algorithm

	% initialize	
	\Population $\leftarrow$ $\emptyset$\;
	\GlobalBest $\leftarrow$ $\emptyset$\;
	\For{$i=1$ $\KwTo$ \PopulationSize} {
		\CurrentVelocity $\leftarrow$ \RandomVelocity{}\;
		\CurrentPosition $\leftarrow$ \RandomPosition{\PopulationSize}\;
		\CurrentBest $\leftarrow$ \CurrentPosition\;
		\If{\Cost{\CurrentBest} $\leq$ \Cost{\GlobalBest}} {
			\GlobalBest $\leftarrow$ \CurrentBest\;
		}
	}	

	% loop
	\While{$\neg$\StopCondition{}} {
		\ForEach{\Particle $\in$ \Population} {
			\CurrentVelocity $\leftarrow$ \UpdateVelocity{\CurrentVelocity, \GlobalBest, \CurrentBest}\;
			\CurrentPosition $\leftarrow$ \UpdatePosition{\CurrentPosition, \CurrentVelocity}\;
			\If{\Cost{\CurrentPosition} $\leq$ \Cost{\CurrentBest}} {
				\CurrentBest $\leftarrow$ \CurrentPosition\;
				\If{\Cost{\CurrentBest} $\leq$ \Cost{\GlobalBest}} {
					\GlobalBest $\leftarrow$ \CurrentBest\;
				}
			}
		}		
	}
	\Return{\GlobalBest}\;
	% end
	\caption{Pseudocode for PSO.}
	\label{alg:pso}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item The number of particles should be low, around 20-40
	\item The speed a particle can move (maximum change in its position per iteration) should be bounded, such as to a percentage of the size of the domain.
	\item The learning factors (biases towards global and personal best positions) should be between 0 and 4, typically 2.
	\item A local bias (local neighborhood) factor can be introduced where neighbors are determined based on Euclidean distance between particle positions.
	\item Particles may leave the boundary of the problem space and may be penalized, be reflected back into the domain or biased to return back toward a position in the problem domain. Alternatively, a wrapping strategy may be used at the edge of the domain creating a loop, torrid or related geometrical structures at the chosen dimensionality.
	\item An inertia or momentum coefficient can be introduced to limit the change in velocity.
\end{itemize}

% Code Listing
% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{pso} provides an example of the Particle Swarm Optimization algorithm implemented in the Ruby Programming Language. 
% problem
The demonstration problem is an instance of a continuous function optimization that seeks $\min f(x)$ where $f=\sum_{i=1}^n x_{i}^2$, $-5.0\leq x_i \leq 5.0$ and $n=3$. The optimal solution for this basin function is $(v_0,\ldots,v_{n-1})=0.0$.
% algorithm
The algorithm is a conservative version of Particle Swarm Optimization based on the seminal papers. The implementation limits the velocity at a pre-defined maximum, and bounds particles to the search space, reflecting their movement and velocity if the bounds of the space are exceeded. Particles are influenced by the best position found as well as their own personal best position. Natural extensions may consider limiting velocity with an inertia coefficient and including a neighborhood function for the particles.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Particle Swarm Optimization in Ruby, label=pso]{../src/algorithms/swarm/pso.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% seminal
Particle Swarm Optimization was described as a stochastic global optimization method for continuous functions in 1995 by Eberhart and Kennedy \cite{Eberhart1995, Kennedy1995}. This work was motivated as an optimization method loosely based on the flocking behavioral models of Reynolds \cite{Reynolds1987}.
% early
Early works included the introduction of inertia \cite{Shi1998} and early study of social topologies in the swarm by Kennedy \cite{Kennedy1999}. 

% 
% Learn More
% 
\subsubsection{Learn More}
% reviews
Poli, Kennedy, and Blackwell provide a modern overview of the field of PSO with detailed coverage of extensions to the baseline technique \cite{Poli2007}. Poli provides a meta-analysis of PSO publications that focus on the application the technique, providing a systematic breakdown on application areas \cite{Poli2008a}. 
% books
An excellent book on Swarm Intelligence in general with detailed coverage of Particle Swarm Optimization is ``Swarm Intelligence'' by Kennedy, Eberhart, and Shi \cite{Kennedy2001}.



