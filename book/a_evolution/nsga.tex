% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Non-dominated Sorting Genetic Algorithm} 
\label{sec:nsga}
\index{Non-dominated Sorting Genetic Algorithm}
\index{NSGA-II}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Non-dominated Sorting Genetic Algorithm, Nondominated Sorting Genetic Algorithm, Fast Elitist Non-dominated Sorting Genetic Algorithm, NSGA, NSGA-II, NSGAII.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
The Non-dominated Sorting Genetic Algorithm is a Multiple Objective Optimization (MOO) algorithm and is an instance of an Evolutionary Algorithm from the field of Evolutionary Computation. Refer to Section~\ref{sec:strategies} for more information and references on Multiple Objective Optimization.
% What are the closely related approaches to a technique?
NSGA is an extension of the Genetic Algorithm for multiple objective function optimization (Section~\ref{sec:genetic_algorithm}).
% related
It is related to other Evolutionary Multiple Objective Optimization Algorithms (EMOO) (or Multiple Objective Evolutionary Algorithms MOEA) such as the Vector-Evaluated Genetic Algorithm (VEGA), Strength Pareto Evolutionary Algorithm (SPEA) (Section~\ref{sec:spea}), and Pareto Archived Evolution Strategy (PAES).
% taxonomy
There are two versions of the algorithm, the classical NSGA and the updated and currently canonical form NSGA-II.

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The objective of the NSGA algorithm is to improve the adaptive fit of a population of candidate solutions to a Pareto front constrained by a set of objective functions.
% What is a techniques plan of action?
The algorithm uses an evolutionary process with surrogates for evolutionary operators including selection, genetic crossover, and genetic mutation. 
% fronts
The population is sorted into a hierarchy of sub-populations based on the ordering of Pareto dominance. Similarity between members of each sub-group is evaluated on the Pareto front, and the resulting groups and similarity measures are used to promote a diverse front of non-dominated solutions.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What is the computational recipe for a technique?
% What are the data structures and representations used in a technique?
Algorithm~\ref{alg:nsga} provides a pseudocode listing of the Non-dominated Sorting Genetic Algorithm II (NSGA-II) for minimizing a cost function. 
% explaination
The \texttt{Sort\-By\-Rank\-And\-Distance} function orders the population into a hierarchy of non-dominated Pareto fronts. The \texttt{Crowding\-Distance\-Assignment} calculates the average distance between members of each front on the front itself. Refer to Deb et al.\ for a clear presentation of the Pseudocode and explanation of these functions \cite{Deb2002}. The \texttt{Crossover\-And\-Mutation} function performs the classical crossover and mutation genetic operators of the Genetic Algorithm. Both the \texttt{Select\-Parents\-By\-Rank\-And\-Distance} and \texttt{Sort\-By\-Rank\-And\-Distance} functions discriminate members of the population first by rank (order of dominated precedence of the front to which the solution belongs) and then distance within the front (calculated by \texttt{Crowding\-Distance\-Assignment}).

\begin{algorithm}[h!t]
	\SetLine  

	% data
	\SetKwData{ProbabilityMutate}{$P_{mutation}$}
	\SetKwData{ProbabilityCrossover}{$P_{crossover}$}
	\SetKwData{Selected}{Selected}
	\SetKwData{Children}{Children}
	\SetKwData{ProblemSize}{ProblemSize}
	\SetKwData{Population}{Population}
	\SetKwData{PopulationSize}{$Population_{size}$}
	\SetKwData{Union}{Union}
	\SetKwData{Fronts}{Fronts}
	\SetKwData{Front}{$Front_i$}
	\SetKwData{Parents}{Parents}
	\SetKwData{LastFront}{$Front_L$}

	% functions
	\SetKwFunction{InitializePopulation}{InitializePopulation}  
	\SetKwFunction{EvaluateAgainstObjectiveFunctions}{EvaluateAgainstObjectiveFunctions} 
	\SetKwFunction{StopCondition}{StopCondition}	
	\SetKwFunction{FastNondominatedSort}{FastNondominatedSort}
	\SetKwFunction{SelectParentsByRank}{SelectParentsByRank}
	\SetKwFunction{SelectParentsByRankAndDistance}{SelectParentsByRankAndDistance}	
	\SetKwFunction{Merge}{Merge}
	\SetKwFunction{Size}{Size}
	\SetKwFunction{Break}{Break}
	\SetKwFunction{CrossoverAndMutation}{CrossoverAndMutation}
	\SetKwFunction{CrowdingDistanceAssignment}{CrowdingDistanceAssignment}
	\SetKwFunction{SortByRankAndDistance}{SortByRankAndDistance}
	
	% I/O
	\KwIn{\PopulationSize, \ProblemSize, \ProbabilityCrossover, \ProbabilityMutate}		
	\KwOut{\Children}
	% Algorithm
	% initialize	
	\Population $\leftarrow$ \InitializePopulation{\PopulationSize, \ProblemSize}\;
	% evaluate
	\EvaluateAgainstObjectiveFunctions{\Population}\;
	\FastNondominatedSort{\Population}\;
	\Selected $\leftarrow$ \SelectParentsByRank{\Population, \PopulationSize}\;
	\Children $\leftarrow$ \CrossoverAndMutation{\Selected, \ProbabilityCrossover, \ProbabilityMutate}\;
	% loop
	\While{$\neg$\StopCondition{}} {
		\EvaluateAgainstObjectiveFunctions{\Children}\;
		\Union $\leftarrow$ \Merge{\Population, \Children}\;
		\Fronts $\leftarrow$ \FastNondominatedSort{\Union}\;		
		% classical loop in paper
		\Parents $\leftarrow \emptyset$\;
		\LastFront $\leftarrow \emptyset$\;
		\ForEach{\Front $\in$ \Fronts}{
			\CrowdingDistanceAssignment{\Front}\;
			\eIf{\Size{\Parents}$+$\Size{\Front} $>$ \PopulationSize}{
			\LastFront $\leftarrow$ $i$\;
				\Break{}\;
			}{
				\Parents $\leftarrow$ \Merge{\Parents, \Front}\;
			}
		}
		\If{\Size{\Parents}$<$\PopulationSize}{
			\LastFront $\leftarrow$ \SortByRankAndDistance{\LastFront}\;
			\For{$P_1$ \KwTo $P_{\PopulationSize - \Size{\LastFront}}$}{
				\Parents $\leftarrow$ $Pi$\;
			}
		}		
		% sample old GA
		\Selected $\leftarrow$ \SelectParentsByRankAndDistance{\Parents, \PopulationSize}\;
		\Population  $\leftarrow$ \Children\;
		\Children $\leftarrow$ \CrossoverAndMutation{\Selected, \ProbabilityCrossover, \ProbabilityMutate}\;
	}
	\Return{\Children}\;
	% end
	\caption{Pseudocode for NSGAII.}
	\label{alg:nsga}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item NSGA was designed for and is suited to continuous function multiple objective optimization problem instances.
	\item A binary representation can be used in conjunction with classical genetic operators such as one-point crossover and point mutation.
	\item A real-valued representation is recommended for continuous function optimization problems, in turn requiring representation specific genetic operators such as Simulated Binary Crossover (SBX) and polynomial mutation \cite{Deb1995}.
\end{itemize}

% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{nsga} provides an example of the Non-dominated Sorting Genetic Algorithm II (NSGA-II) implemented in the Ruby Programming Language.
% problem
The demonstration problem is an instance of continuous multiple objective function optimization called SCH (problem one in \cite{Deb2002}). The problem seeks the minimum of two functions: $f1=\sum_{i=1}^n x_{i}^2$ and $f2=\sum_{i=1}^n (x_{i}-2)^2$, $-10\leq x_i \leq 10$ and $n=1$. The optimal solution for this function are $x \in [0,2]$.
% algorithm
The algorithm is an implementation of NSGA-II based on the presentation by Deb et al.\ \cite{Deb2002}.
%  cfg
The algorithm uses a binary string representation (16 bits per objective function parameter) that is decoded and rescaled to the function domain. The implementation uses a uniform crossover operator and point mutations with a fixed mutation rate of $\frac{1}{L}$, where $L$ is the number of bits in a solution's binary string. 

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=NSGA-II in Ruby, label=nsga]{../src/algorithms/evolutionary/nsgaii.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% seminal
Srinivas and Deb proposed the NSGA inspired by Goldberg's notion of a non-dominated sorting procedure \cite{Srinivas1994}. Goldberg proposed a non-dominated sorting procedure in his book in considering the biases in the Pareto optimal solutions provided by VEGA \cite{Goldberg1989}. Srinivas and Deb's NSGA used the sorting procedure as a ranking selection method, and a fitness sharing niching method to maintain stable sub-populations across the Pareto front.
% extension
Deb et al.\ later extended NSGA to address three criticism of the approach: the $O(mN^3)$ time complexity, the lack of elitism, and the need for a sharing parameter for the fitness sharing niching method \cite{Deb2000, Deb2002}.

% 
% Learn More
% 
\subsubsection{Learn More}
% reviews
% books
Deb provides in depth coverage of Evolutionary Multiple Objective Optimization algorithms in his book, including a detailed description of the NSGA in Chapter 5 \cite{Deb2001}.
